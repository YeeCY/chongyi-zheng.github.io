<meta charset="utf-8" emacsmode="-*- markdown -*-">

**Stabilizing Contrastive RL: Techniques for Offline Goal Reaching**

<p><center>Under Review</center></p>
<p><center><a href="https://chongyi-zheng.github.io/">Chongyi Zheng</a>, &emsp; <a href="https://ben-eysenbach.github.io/">Benjamin Eysenbach</a>, &emsp; <a href="https://homerwalke.com/">Homer Walke</a>, &emsp; <a href="https://patrickyin.me/">Patrick Yin</a>, &emsp; <a href="https://kuanfang.github.io/">Kuan Fang</a>, <br> <a href="https://www.cs.cmu.edu/~rsalakhu/">Ruslan Salakhutdinov</a>, &emsp; <a href="https://people.eecs.berkeley.edu/~svlevine/">Sergey Levine</a></center></p>
<p><center><b><a href="">Paper</a>, &emsp; <a href="">Code</a></b></center></p>

![TODO Teaser](images/teaser.png)

*__Abstract__:
In the same way that the computer vision (CV) and natural language processing (NLP) communities have developed self-supervised methods, reinforcement learning (RL) can be cast as a self-supervised problem: learning to reach any goal, without requiring human-specified rewards or labels. However, actually building a self-supervised foundation for RL faces some important challenges. Building on prior contrastive approaches to this RL problem, we conduct careful ablation experiments and discover that a shallow and wide architecture, combined with careful weight initialization and data augmentation, can significantly boost the performance of these contrastive RL approaches on challenging simulated benchmarks. Additionally, we demonstrate that, with these design decisions, contrastive approaches can solve real-world robotic manipulation tasks, with tasks being specified by a single goal image provided after training.*

Evaluation on Simulated Manipulation Tasks
===============================================================================

Below, we visualize examples of the behavior learned by stable contrastive RL and baselines, GC-IQL and GCBC, on simulated manipulation tasks. The images shown on the top are examples of the initial observation and goal. Note that our method casts reinforcement learning as a self-supervised problem, without using any reward function and successfully solving multi-stage goal-conditioned tasks.

**TASK:** Close the drawer.

![Initial Observation](images/init_obs_drawer.png width="225") ![Goal](images/goal_drawer.png width="225")

![GC-IQL](videos/iql_drawer.mp4 width="225") ![GCBC](videos/gcbc_drawer.mp4 width="225") ![Stable Contrastive RL (Ours)](videos/stable_contrastive_rl_drawer.mp4 width="225")

**TASK:** Pick and place the object from the table.

![Initial Observation](images/init_obs_pick_and_place_table.png width="225") ![Goal](images/goal_pick_and_place_table.png width="225") 

![GC-IQL](videos/iql_pick_and_place_table.mp4 width="225") ![GCBC](videos/gcbc_pick_and_place_table.mp4 width="225") ![Stable Contrastive RL (Ours)](videos/stable_contrastive_rl_pick_and_place_table.mp4 width="225")

**TASK:** Pick and place the object and close the drawer.

![Initial Observation](images/init_obs_pick_and_place_drawer.png width="225") ![Goal](images/goal_pick_and_place_drawer.png width="225")

![GC-IQL](videos/iql_pick_and_place_drawer.mp4 width="225") ![GCBC](videos/gcbc_pick_and_place_drawer.mp4 width="225") ![Stable Contrastive RL (Ours)](videos/stable_contrastive_rl_pick_and_place_drawer.mp4 width="225")

**TASK:** Push block and open the drawer.

![Initial Observation](images/init_obs_push_block_open_drawer.png width="225") ![Goal](images/goal_push_block_open_drawer.png width="225")

![GC-IQL](videos/iql_push_block_open_drawer.mp4 width="225") ![GCBC](videos/gcbc_push_block_open_drawer.mp4 width="225") ![Stable Contrastive RL (Ours)](videos/stable_contrastive_rl_push_block_open_drawer.mp4 width="225")

Failure Cases
-------------------------------------------------------------------------------

For the task below, the agent tries to push the orange block, but fails to close the drawer.

**TASK:** Push block and close the drawer.

![Initial Observation](images/init_obs_push_block_close_drawer.png width="225") ![Goal](images/goal_push_block_close_drawer.png width="225") ![Stable Contrastive RL (Ours)](videos/stable_contrastive_rl_push_block_close_drawer.mp4 width="225")

Evaluation on Real Manipulation Tasks
===============================================================================

We also show examples of the behavior learned by stable contrastive RL and the same baselines on real manipulation tasks. All the methods successfully solves the simplest tasks "reach the eggplant", while stable contrastive RL achives 60% success rates on the other two tasks where baselines fail.

**TASK:** Reach the eggplant.

![Initial Observation](images/init_obs_reach_eggplant.jpg width="225") ![Goal](images/goal_reach_eggplant.jpg width="225")

![GC-IQL](videos/iql_reach_eggplant.mp4 width="225") ![GCBC](videos/gcbc_reach_eggplant.mp4 width="225") ![Stable Contrastive RL (Ours)](videos/stable_contrastive_rl_reach_eggplant.mp4 width="225")

**TASK:** Pick and place the spoon.

![Initial Observation](images/init_obs_pick_and_place_spoon.jpg width="225") ![Goal](images/goal_pick_and_place_spoon.jpg width="225") 

![GC-IQL](videos/iql_pick_and_place_spoon.mp4 width="225") ![GCBC](videos/gcbc_pick_and_place_spoon.mp4 width="225") ![Stable Contrastive RL (Ours)](videos/stable_contrastive_rl_pick_and_place_spoon.mp4 width="225")

**TASK:** Push the can.

![Initial Observation](images/init_obs_push_can.jpg width="225") ![Goal](images/goal_push_can.jpg width="225")

![GC-IQL](videos/iql_push_can.mp4 width="225") ![GCBC](videos/gcbc_push_can.mp4 width="225") ![Stable Contrastive RL (Ours)](videos/stable_contrastive_rl_push_can.mp4 width="225")


--------------------------

<!-- Markdeep: --><style class="fallback">body{visibility:hidden;white-space:pre;font-family:monospace}</style><script src="markdeep.min.js" charset="utf-8"></script><script src="https://morgan3d.github.io/markdeep/latest/markdeep.min.js" charset="utf-8"></script><script>window.alreadyProcessedMarkdeep||(document.body.style.visibility="visible")</script>
